{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic0.csv\n",
      "severe_toxic0.csv\n",
      "obscene0.csv\n",
      "threat0.csv\n",
      "insult0.csv\n",
      "identity_hate0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:50<03:20, 50.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic1.csv\n",
      "severe_toxic1.csv\n",
      "obscene1.csv\n",
      "threat1.csv\n",
      "insult1.csv\n",
      "identity_hate1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [01:44<02:37, 52.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic2.csv\n",
      "severe_toxic2.csv\n",
      "obscene2.csv\n",
      "threat2.csv\n",
      "insult2.csv\n",
      "identity_hate2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [02:34<01:42, 51.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic3.csv\n",
      "severe_toxic3.csv\n",
      "obscene3.csv\n",
      "threat3.csv\n",
      "insult3.csv\n",
      "identity_hate3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [03:24<00:51, 51.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic4.csv\n",
      "severe_toxic4.csv\n",
      "obscene4.csv\n",
      "threat4.csv\n",
      "insult4.csv\n",
      "identity_hate4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:13<00:00, 50.78s/it]\n"
     ]
    }
   ],
   "source": [
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\" ,\"threat\", \"insult\" ,\"identity_hate\"]\n",
    "#Iterate for 5 samples\n",
    "for i in tqdm(range(0,5)):\n",
    "    #load test datasets\n",
    "    df_test = pd.read_csv('clean_test_wo_capital.csv')\n",
    "    df_test = df_test.fillna(\"\")\n",
    "    del df_test[\"Unnamed: 0\"]\n",
    "    \n",
    "    #Iterate for 6 classes\n",
    "    for cls in classes:\n",
    "        #load each sampled training datasets\n",
    "        s = cls + str(i) + '.csv'\n",
    "        print(s)\n",
    "        df_train = pd.read_csv(s)\n",
    "        df_train = df_train.fillna(\"\")\n",
    "        #Combine test and train datasets\n",
    "        df_all = pd.concat([df_train,df_test])\n",
    "\n",
    "        #Build feature vector\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 20000)\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 20000)\n",
    "        \"\"\"\n",
    "        features = vectorizer.fit_transform(df_all[\"comment_text\"])\n",
    "\n",
    "        #Training and predict \n",
    "        #You can try another model by modifyig here\n",
    "        nb = MultinomialNB()\n",
    "        nb = nb.fit(features[0:df_train.shape[0]] , df_train[cls])\n",
    "        prob = nb.predict_proba(features[df_train.shape[0]:])[:,1]\n",
    "        df_test[cls] = prob\n",
    "    \n",
    "    #Generate submission file for each sample\n",
    "    #Average thame later\n",
    "    del df_test[\"comment_text\"]\n",
    "    df_test.set_index('id',inplace=True) \n",
    "    df_test.to_csv(\"submission\" + \"_nb_20000\" + str(i) +  \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
