{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we do for text pre-processing\n",
    "\n",
    "・Convert text to lowercase – This is to avoid distinguish between words simply on case.\n",
    "\n",
    "・Remove Number – Numbers may or may not be relevant to our analyses. Usually it does not carry any importance in sentiment analysis\n",
    "\n",
    "・Remove Punctuation – Punctuation can provide grammatical context which supports understanding. For bag of words based sentiment analysis punctuation does not add value.\n",
    "\n",
    "・Remove English stop words – Stop words are common words found in a language. Words like for, of, are etc are common stop words.\n",
    "\n",
    "・Remove Own stop words(if required) – Along with English stop words, we could instead or in addition remove our own stop words. The choice of own stop word might depend on the domain of discourse, and might not become apparent until we’ve done some analysis.\n",
    "\n",
    "・Strip white space – Eliminate extra white spaces.\n",
    "\n",
    "・Stemming – Transforms to root word. Stemming uses an algorithm that removes common word endings for English words, such as “es”, “ed” and “’s”. For example i.e., 1) “computer” & “computers” become “comput”\n",
    "\n",
    "・Lemmatisation – transform to dictionary base form i.e., “produce” & “produced” become “produce”\n",
    "\n",
    "・Sparse terms – We are often not interested in infrequent terms in our documents. Such “sparse” terms should be removed from the document term matrix.\n",
    "\n",
    "Reference:https://datascience.stackexchange.com/questions/11402/preprocessing-text-before-use-rnn/11421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "from nltk import tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import stem\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\" More I can't make any real suggestions on im...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation Why the edits made under my userna...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \" More I can't make any real suggestions on im...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"comment_text\"] = df_train[\"comment_text\"].str.replace(\"\\n\",\" \") #\\n removal\n",
    "df_test[\"comment_text\"] = df_test[\"comment_text\"].str.replace(\"\\n\",\" \")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC ==    The title is fine as it is, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\"    == Sources ==    * Zawe Ashton on Lapland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC ==    The title is fine as it is, ...\n",
       "2  00013b17ad220c46  \"    == Sources ==    * Zawe Ashton on Lapland...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n"
     ]
    }
   ],
   "source": [
    "s = df_train[\"comment_text\"][0]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\n"
     ]
    }
   ],
   "source": [
    "# Captal to lower\n",
    "s = s.lower()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms just closure on some gas after i voted at new york dolls fac and please don't remove the template from the talk page since i'm retired now\n"
     ]
    }
   ],
   "source": [
    "#Number removal\n",
    "def remove_number(s):\n",
    "    s = re.sub(r'[0-9]', \"\", s)\n",
    "    return s\n",
    "s = remove_number(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation why the edits made under my username hardcore metallica fan were reverted they weren't vandalisms just closure on some gas after i voted at new york dolls fac and please don't remove the template from the talk page since i'm retired now\n"
     ]
    }
   ],
   "source": [
    "#Punctuation removal\n",
    "def remove_punctuation(s):\n",
    "    s = s.replace('\"',\"\")\n",
    "    s = s.replace(',',\"\")\n",
    "    s = s.replace('.',\"\")\n",
    "    s = s.replace(\"?\",\"\")\n",
    "    s = s.replace(\"!\",\"\")\n",
    "    s = s.replace(\"[\",\"\")\n",
    "    s = s.replace(\"]\",\"\")\n",
    "    s = s.replace(\"{\",\"\")\n",
    "    s = s.replace(\"}\",\"\")\n",
    "    s = s.replace(\"(\",\"\")\n",
    "    s = s.replace(\")\",\"\")\n",
    "    s = s.replace(\":\",\"\")\n",
    "    s = s.replace(\";\",\"\")\n",
    "    return s\n",
    "s = remove_punctuation(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['explanation', 'why', 'the', 'edit', 'make', 'under', 'my', 'username', 'hardcore', 'metallica', 'fan', 'be', 'reverted?', 'they', \"weren't\", 'vandalisms', 'just', 'closure', 'on', 'some', 'gas', 'after', 'i', 'vote', 'at', 'new', 'york', 'dolls', 'fac', 'and', 'please', \"don't\", 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', \"i'm\", 'retire', 'now']\n"
     ]
    }
   ],
   "source": [
    "#Lematization and stemming\n",
    "words = s.split()\n",
    "lemmatizer = stem.WordNetLemmatizer()\n",
    "for i in range(len(words)):\n",
    "    words[i] = lemmatizer.lemmatize(words[i],pos='v')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ain', 'yourselves', 'your', \"couldn't\", 'an', 'were', 'i', 'have', \"it's\", 'won', 'which', \"mustn't\", 'then', \"didn't\", \"you'll\", 'in', 'it', 'is', 'on', 'who', 'further', 'once', 'other', 'couldn', 'itself', 'our', 'those', 'again', 'few', 'any', \"she's\", 'ours', \"won't\", 'for', 'into', 'through', 'why', 'hasn', 'own', 'while', 'her', 'mustn', 'didn', 'here', 'should', 'now', \"hasn't\", 'these', 'above', 'nor', 'my', 'how', \"you're\", 'isn', 'if', 'his', 'himself', 'by', 've', 'not', 'are', \"shouldn't\", 'their', 'a', 'both', 'mightn', 'him', 'most', \"aren't\", \"wouldn't\", 'myself', 'they', 'to', 'only', \"hadn't\", \"mightn't\", \"needn't\", 'there', 'same', \"you'd\", 'or', \"isn't\", 'more', 'herself', 'ourselves', 'you', 'up', 'all', 'just', \"should've\", 'each', 'been', 'its', 'll', 'doesn', 'has', 'from', 'but', 'over', 'at', 'aren', 'after', 'doing', 'where', \"don't\", 'and', 'down', 'as', 'themselves', \"that'll\", 'am', \"shan't\", 'yours', 'so', 'such', 'too', 'does', 'before', \"you've\", 'off', 'ma', 'will', 'during', 'that', 'me', 'this', \"weren't\", 'yourself', 'weren', \"wasn't\", 'very', 'when', 'until', 'can', 'with', 'what', 'do', 't', 'hers', 'no', 'y', 'shan', 'wouldn', 'had', 'having', 'under', 'don', 'o', 'out', 'below', 'haven', 'did', 'shouldn', 'she', 'be', 'wasn', 'hadn', 'whom', 're', 'd', 's', 'being', 'because', 'was', 'he', 'about', 'than', 'some', 'needn', 'of', \"doesn't\", 'we', 'against', 'theirs', 'the', \"haven't\", 'm', 'them', 'between'}\n"
     ]
    }
   ],
   "source": [
    "#Stop word removal\n",
    "#Show stopword list\n",
    "#We can add our own stop word by adding own stop word to that list\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['explanation', 'edit', 'make', 'username', 'hardcore', 'metallica', 'fan', 'reverted?', 'vandalisms', 'closure', 'gas', 'vote', 'new', 'york', 'dolls', 'fac', 'please', 'remove', 'template', 'talk', 'page', 'since', \"i'm\", 'retire']\n"
     ]
    }
   ],
   "source": [
    "#Stop word removal\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "meaningful_words = [w for w in words if not w in stops] \n",
    "print(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Capital and lower count as different.\n",
    "def comment_to_words_1(comment):\n",
    "    lemmatizer = stem.WordNetLemmatizer()\n",
    "    comment = remove_number(comment)\n",
    "    comment = remove_punctuation(comment)\n",
    "    words = comment.split()\n",
    "    for i in range(len(words)):\n",
    "        words[i] = lemmatizer.lemmatize(words[i],pos='v')\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    # Join the words back into one string separated by space and return the result.\n",
    "    return( \" \".join( meaningful_words ))   \n",
    "\n",
    "#Capital and lower count as same and only include letters\n",
    "def comment_to_words_2(comment):\n",
    "    lemmatizer = stem.WordNetLemmatizer()\n",
    "    comment = remove_number(comment)\n",
    "    letters_only = remove_punctuation(comment)\n",
    "    words = letters_only.lower().split()\n",
    "    for i in range(len(words)):\n",
    "        words[i] = lemmatizer.lemmatize(words[i],pos='v')\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    # Join the words back into one string separated by space and return the result.\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation Why edit make username Hardcore Metallica Fan revert They vandalisms closure GAs I vote New York Dolls FAC And please remove template talk page since I'm retire\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_to_words_1(df_train[\"comment_text\"][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [01:16<00:00, 2095.61it/s]\n"
     ]
    }
   ],
   "source": [
    "#tqdm is the library to show the progress of the iteration. You can omit if you want\n",
    "from tqdm import tqdm\n",
    "clean_comment_train = []\n",
    "for i in tqdm(range(df_train.shape[0])):\n",
    "    clean_comment_train.append(comment_to_words_1(df_train[\"comment_text\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153164/153164 [01:13<00:00, 2097.88it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_comment_test = []\n",
    "for i in tqdm(range(df_test.shape[0])):\n",
    "    clean_comment_test.append(comment_to_words_1(df_test[\"comment_text\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#You can save as new csv file as preprocessing takes time\n",
    "df_train[\"comment_text\"] = clean_comment_train\n",
    "df_train.to_csv( 'clean_train.csv' )\n",
    "df_test[\"comment_text\"] = clean_comment_test\n",
    "df_test.to_csv( 'clean_test.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [01:16<00:00, 2085.26it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_comment_train = []\n",
    "for i in tqdm(range(df_train.shape[0])):\n",
    "    clean_comment_train.append(comment_to_words_2(df_train[\"comment_text\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153164/153164 [01:11<00:00, 2143.80it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_comment_test = []\n",
    "for i in tqdm(range(df_test.shape[0])):\n",
    "    clean_comment_test.append(comment_to_words_2(df_test[\"comment_text\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Without capital letter\n",
    "df_train[\"comment_text\"] = clean_comment_train\n",
    "df_train.to_csv( 'clean_train_wo_capital.csv' )\n",
    "df_test[\"comment_text\"] = clean_comment_test\n",
    "df_test.to_csv( 'clean_test_wo_capital.csv' )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
